{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the first notebook of the Un4CHANate project.\n",
    "\n",
    "In this notebook you will chunk and clean dataset from the InternetArchive 4Plebs (https://archive.org/details/4plebs-org-data-dump-2024-01). Through the chunking process, the files are divided into smaller, more manageable pieces, making them small enough to be opened and processed on a standard computer without requiring high-performance hardware.\n",
    "\n",
    "If you are interested in testing our second notebook ---for data analysis--- a demo csv file is added onto this Github repository.\n",
    "\n",
    "This notebook extracts the csv file from 4plebs to a smaller csv file only containing the timestamp, comment in the timeframe of interest for your own analysis.\n",
    "\n",
    "**STEP I:**\n",
    "\n",
    "Download your chosen dataset at https://archive.org/details/4plebs-org-data-dump-2024-01\n",
    "\n",
    "*Categories:*\n",
    "- /b/: Random (the infamous anything-goes board).\n",
    "- /v/: Video games.\n",
    "- /pol/: Politically incorrect.\n",
    "- /a/: Anime & manga.\n",
    "\n",
    "**STEP II:** \n",
    "\n",
    "Write down the the start- and end-date of the timeframe of interest for your analysis (YYYY-MM-DD). This code cell will give you the UNIX values back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Fill your timeframe of interest in Year-Month-Day format, this code cell will give you the unix-values needed to find the correct timeframe in the larger CSV file.\n",
    "\n",
    "def convert_to_unix(date_string, date_format=\"%Y-%m-%d\"):\n",
    "\n",
    "    try:\n",
    "        # Parse the date string into a datetime object\n",
    "        date_obj = datetime.strptime(date_string, date_format)\n",
    "        # Convert the datetime object to a Unix timestamp\n",
    "        unix_timestamp = int(date_obj.timestamp())\n",
    "        return unix_timestamp\n",
    "    except ValueError as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Prompt the user for start and end dates\n",
    "start_date = input(\"Enter the start date (format: YYYY-MM-DD): \")\n",
    "end_date = input(\"Enter the end date (format: YYYY-MM-DD): \")\n",
    "\n",
    "# Convert the provided dates to Unix timestamps\n",
    "timestamp_start = int(convert_to_unix(start_date))\n",
    "timestamp_end = int(convert_to_unix(end_date))\n",
    "\n",
    "# Print the results\n",
    "print(f\"Date of start = {start_date}\")\n",
    "print(f\"timestamp_start = {timestamp_start}\")\n",
    "print(f\"Date of end = {end_date}\")\n",
    "print(f\"timestamp_end = {timestamp_end}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP III:** \n",
    "\n",
    "Write down the correct path which redirects to the CSV containing the dataset downloaded on 4plebs. Create a folder and fill in the path where the file chuncks will show up.\n",
    "\n",
    "Please note that by changing the columns of interest the notebooks will not work adequately.\n",
    "\n",
    "This process will take some time, you can wait for all the chuncks to be processed.\n",
    "Another option would be to open the processed chunks and assess if the number in the 'time' column is corresponding within your timeframe. To find the the number tied to the timeframe read the UNIX number at STEP II.\n",
    "\n",
    "**Note** 1 You can change the 'chunck_size', the smaller the number the less time it will take your computer to create a chunck, but you will end up with more chuncks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "input_file = r'C:\\Users'  # Direct to the csv file.\n",
    "output_folder = r'C:\\Users' # Direct to the folder in which you want the chuncked files\n",
    "\n",
    "chunk_size = 10000000 # Number of rows per chunk\n",
    "columns_to_extract = [4, 22]  # Columns containing the timestamp and comment string\n",
    "pattern = r\">>\"  # Regex pattern to match strings containing '>>'\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Process the file in chunks\n",
    "chunks = pd.read_csv(\n",
    "    input_file,\n",
    "    engine='python',\n",
    "    chunksize=chunk_size,\n",
    "    on_bad_lines='skip',\n",
    "    delimiter=',',\n",
    "    quoting=3\n",
    ")\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    try:\n",
    "        # Extract specified columns and drop missing values\n",
    "        selected_columns = chunk.iloc[:, columns_to_extract].copy()\n",
    "        selected_columns.columns = ['time', 'comment']  # Rename the columns for clarity\n",
    "        selected_columns = selected_columns.dropna()\n",
    "\n",
    "        # Filter out rows where 'comment' contains '>>'\n",
    "        cleaned_data = selected_columns[~selected_columns['comment'].str.contains(pattern, na=False)]\n",
    "\n",
    "        # Strip whitespace or quotes from the 'time' column and convert to integers\n",
    "        cleaned_data['time'] = cleaned_data['time'].astype(str).str.strip(' \"')\n",
    "        cleaned_data['time'] = pd.to_numeric(cleaned_data['time'], errors='coerce', downcast='integer')\n",
    "\n",
    "        # Drop rows where conversion resulted in NaN\n",
    "        cleaned_data = cleaned_data.dropna(subset=['time'])\n",
    "\n",
    "        # Convert the 'time' column to integers after cleaning\n",
    "        cleaned_data['time'] = cleaned_data['time'].astype(int)\n",
    "\n",
    "        # Define the output file path for this chunk\n",
    "        output_file = os.path.join(output_folder, f\"chunk_{i + 1}.csv\")\n",
    "\n",
    "        # Save the cleaned chunk to a CSV file\n",
    "        cleaned_data.to_csv(output_file, index=False, header=True)\n",
    "        print(f\"Cleaned chunk {i + 1} saved to {output_file}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing chunk {i + 1}: {e}\")\n",
    "\n",
    "print(\"All chunks processed and saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP IV:**\n",
    "\n",
    "Fill in the 'filtered_folder' with the folder where you want to save the filtered chunks.\n",
    "\n",
    "Fill in the 'final_output_file' with the path and name for the final CSV.\n",
    "\n",
    "This code cell below will merge the CSV chuncks into one CSV file only containing the rows that are within your timeframe.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def filter_chunks_by_timestamp(input_folder, output_folder, timestamp_start, timestamp_end):\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for file in os.listdir(input_folder):\n",
    "        if file.endswith('.csv'):\n",
    "            file_path = os.path.join(input_folder, file)\n",
    "            try:\n",
    "                # Read the CSV file\n",
    "                df = pd.read_csv(file_path)\n",
    "\n",
    "                # Ensure the first column is cleaned and converted to integers\n",
    "                df.iloc[:, 0] = df.iloc[:, 0].astype(str).str.strip(' \"')\n",
    "                df.iloc[:, 0] = pd.to_numeric(df.iloc[:, 0], errors='coerce', downcast='integer')\n",
    "\n",
    "                # Drop rows where conversion resulted in NaN\n",
    "                df = df.dropna(subset=[df.columns[0]])\n",
    "\n",
    "                # Convert the column to integers after cleaning\n",
    "                df.iloc[:, 0] = df.iloc[:, 0].astype(int)\n",
    "\n",
    "                # Filter rows based on the timestamp range\n",
    "                filtered_df = df[(df.iloc[:, 0] >= timestamp_start) & (df.iloc[:, 0] <= timestamp_end)]\n",
    "\n",
    "                # If there are matching rows, save the filtered chunk to the output folder\n",
    "                if not filtered_df.empty:\n",
    "                    output_path = os.path.join(output_folder, file)\n",
    "                    filtered_df.to_csv(output_path, index=False)\n",
    "                    print(f\"Filtered chunk saved: {output_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "def merge_filtered_chunks(filtered_folder, output_file):\n",
    "  \n",
    "    dataframes = []\n",
    "\n",
    "    for file in os.listdir(filtered_folder):\n",
    "        if file.endswith('.csv'):\n",
    "            file_path = os.path.join(filtered_folder, file)\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                dataframes.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "    if dataframes:\n",
    "        merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "        merged_df.to_csv(output_file, index=False)\n",
    "        print(f\"Merged CSV saved to {output_file}\")\n",
    "    else:\n",
    "        print(\"No CSV files found to merge.\")\n",
    "\n",
    "# Fill in the missing values\n",
    "input_folder = output_folder\n",
    "filtered_folder = r\"C:\\Users\"  # Replace with the folder where you want to save the filtered chunks\n",
    "final_output_file = r\"C:\\Users.csv\"  # Replace with the path and name for the final CSV\n",
    "\n",
    "filter_chunks_by_timestamp(input_folder, filtered_folder, timestamp_start, timestamp_end)\n",
    "merge_filtered_chunks(filtered_folder, final_output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATA_LOADER_4CHAN** Completed! Now proceed to the second notebook to analyze the the dataset!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
